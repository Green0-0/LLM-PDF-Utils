# Statistical Learning

## 2.1 What Is Statistical Learning?

In order to motivate our study of statistical learning, we begin with a simple example. Suppose that we are statistical consultants hired by a client to investigate the association between advertising and sales of a particular product. The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper. The data are displayed in Figure 2.1. It is not possible for our client to directly increase sales of the product. On the other hand, they can control the advertising expenditure in each of the three media. Therefore, if we determine that there is an association between advertising and sales, then we can instruct our client to adjust advertising budgets, thereby indirectly increasing sales. In other words, our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.

In this setting, the advertising budgets are input variables while sales is an output variable. The input variables are typically denoted using the symbol $X$, with a subscript to distinguish them. So $X_1$ might be the TV output budget, $X_2$ the radio budget, and $X_3$ the newspaper budget. The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable—in this case, sales—is often called the response or dependent variable, and is typically denoted using the symbol $Y$. Throughout this book, we will use all of these terms interchangeably.

More generally, suppose that we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots, X_p$. We assume that there is some relationship between $Y$ and $X = (X_1, X_2, \ldots, X_p)$, which can be written in the very general form

$$
Y = f(X) + \epsilon. \tag{2.1}
$$

© Springer Nature Switzerland AG 2022

G. James et al., *An Introduction to Statistical Learning*, Springer Texts in Statistics.

https://doi.org/10.1007/978-3-031-38747-0_2![Figure 2.1](image1.png)

**FIGURE 2.1.** The advertising data set. The plot displays sales, in thousands of units, as a function of TV, radio, and newspaper budgets, in thousands of dollars, for 200 different markets. In each plot we show the simple least squares fit of sales to that variable, as described in Chapter 3. In other words, each blue line represents a simple model that can be used to predict sales using TV, radio, and newspaper, respectively.

Here f is some fixed but unknown function of X1, ..., XP, and ε is a random error term, which is independent of X and has mean zero. In this formulation, f represents the systematic information that X provides about Y.

![Figure 2.2](image2.png)

**FIGURE 2.2.** The income data set. Left: The red dots are the observed values of income (in thousands of dollars) and years of education for 30 individuals. Right: The blue curve represents the true underlying relationship between income and years of education, which is generally unknown (but is known in this case because the data were simulated). The black lines represent the error associated with each observation. Note that some errors are positive (if an observation lies above the blue curve) and some are negative (if an observation lies below the curve). Overall, these errors have approximately mean zero.

As another example, consider the left-hand panel of Figure 2.2, a plot of income versus years of education for 30 individuals in the income data set. The plot suggests that one might be able to predict income using years of education. However, the function f that connects the input variable to the error term systematicoutput variable is in general unknown. In this situation one must estimate f based on the observed points. Since Income is a simulated data set, f is known and is shown by the blue curve in the right-hand panel of Figure 2.2. The vertical lines represent the error terms e. We note that some of the 30 observations lie above the blue curve and some lie below it; overall, the errors have approximately mean zero.

In general, the function f may involve more than one input variable. In Figure 2.3 we plot income as a function of years of education and seniority. Here f is a two-dimensional surface that must be estimated based on the observed data.

In essence, statistical learning refers to a set of approaches for estimating f. In this chapter we outline some of the key theoretical concepts that arise in estimating f, as well as tools for evaluating the estimates obtained.

### 2.1.1 Why Estimate f?

There are two main reasons that we may wish to estimate f: prediction and inference. We discuss each in turn.

#### Prediction

In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y using

$$
\hat{Y} = \hat{f}(X),
$$

where \(\hat{f}\) represents our estimate for f, and \(\hat{Y}\) represents the resulting prediction for Y. In this setting, \(\hat{f}\) is often treated as a black box, in the sense that one is not typically concerned with the exact form of f, provided that it yields accurate predictions for \(\hat{Y}\).

As an example, suppose that \(X_1, \ldots, X_p\) are characteristics of a patient's blood sample that can be easily measured in a lab, and Y is a variable encoding the patient's risk for a severe adverse reaction to a particular drug. It is natural to seek to predict Y using X, since we can then avoid giving the drug in question to patients who are at high risk of an adverse reaction—that is, patients for whom the estimate of \(\hat{Y}\) is high.

The accuracy of \(\hat{Y}\) as a prediction for Y depends on two quantities, which we will call the reducible error and the irreducible error. In general, \(\hat{f}\) will not be a perfect estimate for f, and this inaccuracy will introduce some error. This error is reducible because we can potentially improve the accuracy of \(\hat{f}\) by using the most appropriate statistical learning technique to estimate f. However, even if it were possible to form a perfect estimate for f, so that our estimated response took the form \(\hat{Y} = \hat{f}(X)\), our prediction would still have some error in it! This is because Y is also a function of e, which, by definition, cannot be predicted using X. Therefore, variability associated with e also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate f, we cannot reduce the error introduced by e.

Why is the irreducible error larger than zero? The quantity e may contain unmeasured variables that are useful in predicting Y: since we don't![Graph](image_url)

**FIGURE 2.3.** The plot displays income as a function of years of education and seniority in the income data set. The blue surface represents the true underlying relationship between income and years of education and seniority, which is known since the data are simulated. The red dots indicate the observed values of these quantities for 30 individuals.

measure them, \( f \) cannot use them for its prediction. The quantity \( \epsilon \) may also contain unmeasurable variation. For example, the risk of an adverse reaction might vary for a given patient on a given day, depending on manufacturing variation in the drug itself or the patient’s general feeling of well-being on that day.

Consider a given estimate \( \hat{f} \) and a set of predictors \( X \), which yields the prediction \( \hat{Y} = \hat{f}(X) \). Assume for a moment that both \( f \) and \( X \) are fixed, so that the only variability comes from \( \epsilon \). Then, it is easy to show that

\[ \mathbb{E}(Y - \hat{Y})^2 = \mathbb{E}[f(X) + \epsilon - \hat{f}(X)]^2 \]

\[ = \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{\text{Var}(\epsilon)}_{\text{Irreducible}} \tag{2.3} \]

where \( \mathbb{E}(Y - \hat{Y})^2 \) represents the average, or expected value, of the squared difference between the predicted and actual value of \( Y \), and \( \text{Var}(\epsilon) \) represents the variance associated with the error term \( \epsilon \).

The focus of this book is on techniques for estimating \( f \) with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for \( Y \). This bound is almost always unknown in practice.

### Inference

We are often interested in understanding the association between \( Y \) and \( X_1, \ldots, X_p \). In this situation we wish to estimate \( f \), but our goal is not necessarily to make predictions for \( Y \). Now \( \hat{f} \) cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:

- expected value
- variance- **Which predictors are associated with the response?** It is often the case that only a small fraction of the available predictors are substantially associated with Y. Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application.

- **What is the relationship between the response and each predictor?** Some predictors may have a positive relationship with Y, in the sense that larger values of the predictor are associated with larger values of Y. Other predictors may have the opposite relationship. Depending on the complexity of f, the relationship between the response and a given predictor may also depend on the values of the other predictors.

- **Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?** Historically, most methods for estimating f have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.

In this book, we will see a number of examples that fall into the prediction setting, the inference setting, or a combination of the two.

For instance, consider a company that is interested in conducting a direct-marketing campaign. The goal is to identify individuals who are likely to respond positively to a mailing, based on observations of demographic variables measured on each individual. In this case, the demographic variables serve as predictors, and response to the marketing campaign (either positive or negative) serves as the outcome. The company is not interested in obtaining a deep understanding of the relationships between each individual predictor and the response; instead, the company simply wants to accurately predict the response using the predictors. This is an example of modeling for prediction.

In contrast, consider the advertising data illustrated in Figure 2.1. One may be interested in answering questions such as:

- Which media are associated with sales?
- Which media generate the biggest boost in sales? or
- How large of an increase in sales is associated with a given increase in TV advertising?

This situation falls into the inference paradigm. Another example involves modeling the brand of a product that a customer might purchase based on variables such as price, store location, discount levels, competition price, and so forth. In this situation one might really be most interested in the association between each variable and the probability of purchase. For instance, to what extent is the product's price associated with sales? This is an example of modeling for inference.

Finally, some modeling could be conducted both for prediction and inference. For example, in a real estate setting, one may seek to relate valuesof homes to inputs such as crime rate, zoning, distance from a river, air quality, schools, income level of community, size of houses, and so forth. In this case one might be interested in the association between each individual input variable and housing price - for instance, how much extra will a house be worth if it has a view of the river? This is an inference problem. Alternatively, one may simply be interested in predicting the value of a home given its characteristics: is this house under- or over-valued? This is a prediction problem.

Depending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating f may be appropriate. For example, linear models allow for relatively simple and interpretable inference, but may not yield as accurate predictions as some other approaches. In contrast, some of the highly non-linear approaches that we discuss in the later chapters of this book can potentially provide quite accurate predictions for Y, but this comes at the expense of a less interpretable model for which inference is more challenging.

### 2.1.2 How Do We Estimate f?

Throughout this book, we explore many linear and non-linear approaches for estimating f. However, these methods generally share certain characteristics. We provide an overview of these shared characteristics in this section. We will always assume that we have observed a set of n different data points. For example in Figure 2.2 we observed n = 30 data points. These observations are called the training data because we will use these observations to train, or teach, our method how to estimate f. Let x_ij data represent the value of the jth predictor, or input, for observation i, where i = 1, 2, ..., n and j = 1, 2, ..., p. Correspondingly, let y_i represent the response variable for the ith observation. Then our training data consist of {(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)} where x_i = (x_i1, x_i2, ..., x_ip)^T.

Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function f. In other words, we want to find a function f such that Y ≈ f(X) for any observation (X, Y). Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric. We now briefly discuss these two types of approaches.

#### Parametric Methods

Parametric methods involve a two-step model-based approach.

1. First, we make an assumption about the functional form, or shape, of f. For example, one very simple assumption is that f is linear in X:

   f(X) = β_0 + β_1X_1 + β_2X_2 + ... + β_pX_p. (2.4)

   This is a linear model, which will be discussed extensively in Chapter 3. Once we have assumed that f is linear, the problem of estimating f is greatly simplified. Instead of having to estimate an entirely arbitrary p-dimensional function f(X), one only needs to estimate the p + 1 coefficients β_0, β_1, ..., β_p.

![linear model](https://example.com/linear_model.png)

![parametric non-parametric](https://example.com/parametric_non_parametric.png)![Graph](image_url)

**FIGURE 2.4.** A linear model fit by least squares to the Income data from Figure 2.3. The observations are shown in red, and the yellow plane indicates the least squares fit to the data.

2. After a model has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model (2.4), we need to estimate the parameters β0, β1, ..., βp. That is, we want to find values of these parameters such that

Y ≈ β0 + β1X1 + β2X2 + ... + βpXp.

The most common approach to fitting the model (2.4) is referred to as (ordinary) least squares, which we discuss in Chapter 3. However, least squares is one of many possible ways to fit the linear model. In Chapter 6, we discuss other approaches for estimating the parameters in (2.4).

The model-based approach just described is referred to as parametric; it reduces the problem of estimating f down to one of estimating a set of parameters. Assuming a parametric form for f simplifies the problem of estimating f because it is generally much easier to estimate a set of parameters, such as β0, β1, ..., βp in the linear model (2.4), than it is to fit an entirely arbitrary function f. The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of f. If the chosen model is too far from the true f, then our estimate will be poor. We can try to address this problem by choosing flexible models that can fit many different possible functional forms for f. But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely. These issues are discussed throughout this book.

Figure 2.4 shows an example of the parametric approach applied to the Income data from Figure 2.3. We have fit a linear model of the form

income ≈ β0 + β1 × education + β2 × seniority.

least squares

flexible

flexible

overfitting

overfitting

noise

noise